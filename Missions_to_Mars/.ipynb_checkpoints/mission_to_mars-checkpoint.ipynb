{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to chromedriver\n",
    "executable_path = {\"executable_path\": \"C:/bin/chromedriver\"}\n",
    "browser=Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build query URL for NASA news (using Splinter to scrape)\n",
    "url = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# Collect the latest News Title and Paragraph Text\n",
    "# Assign the text to variables that you can reference later\n",
    "news_title = soup.find('div', class_='bottom_gradient').text\n",
    "news_p = soup.find('div', class_='article_teaser_body').text\n",
    "\n",
    "print(news_title)\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build query URL for JPL featured [Mars] image - use Splinter to scrape\n",
    "url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# Find the image url for the current Featured Mars Image\n",
    "#featured_image_url = soup.find('a', class_='carousel_item', style) ## produces error\n",
    "#featured_image_url = soup.find('a', class_='carousel_item')['style'] ## produces error\n",
    "#featured_image_url = soup.find('article', class_='carousel_item')['style'] ## includes unneeded control text\n",
    "\n",
    "# Removed uneeded control text\n",
    "#featured_image_url = soup.find('article', class_='carousel_item')['style'].replace('background-image: url(','')\n",
    "#featured_image_url = soup.find('article', class_='carousel_item')['style'].replace('background-image: url(','').replace(');', '')  ## includes quotes\n",
    "\n",
    "# Find the image url for the current Featured Mars Image and remove unneeded control text\n",
    "featured_image_url = soup.find('article', class_='carousel_item')['style'].\\\n",
    "                        replace('background-image: url(','').\\\n",
    "                        replace(');', '')[1:-1]\n",
    "\n",
    "\n",
    "\n",
    "# image url from html only makes sense when referenced in source site - add source site to front of url\n",
    "featured_image_url = \"https://www.jpl.nasa.gov\" + featured_image_url\n",
    "\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars Facts webpage and use Pandas to scrape the table\n",
    "results = pd.read_html(\"https://space-facts.com/mars/\")\n",
    "print(f'Total tables: {len(results)}')\n",
    "print()\n",
    "facts_df1 = results[0]\n",
    "facts_df2 = results[1]\n",
    "facts_df3 = results[2]\n",
    "\n",
    "# Use Pandas to convert the data to a HTML table string\n",
    "facts_html1 = facts_df1.to_html()\n",
    "facts_html2 = facts_df2.to_html()\n",
    "facts_html3 = facts_df3.to_html()\n",
    "\n",
    "#print(facts_html1)\n",
    "print(facts_html2)\n",
    "#print(facts_html3)\n",
    "\n",
    "#facts_df = pd.DataFrame(results)\n",
    "#facts_html = facts_df.to_html()\n",
    "#print(facts_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere ',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere ',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere ',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere ',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visit the USGS Astrogeology site to obtain high resolution images for each of Mar's hemispheres\n",
    "# Build query URL for page in USGS Astrogeology site that has pics of Mars' hemispheres\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "results = soup.find ('div', class_='result-list')\n",
    "images = results.find_all('div',{'class':'item'})\n",
    "\n",
    "hemisphere_image_urls=[]\n",
    "\n",
    "for i in images:\n",
    "    title = i.find(\"h3\").text\n",
    "    title = title.replace(\"Enhanced\", \"\")\n",
    "    end_link = i.find(\"a\")[\"href\"]\n",
    "    image_link = \"https://astrogeology.usgs.gov/\" + end_link    \n",
    "    browser.visit(image_link)\n",
    "    html_hemispheres = browser.html\n",
    "    soup=bs(html_hemispheres, \"html.parser\")\n",
    "    downloads = soup.find(\"div\", class_=\"downloads\")\n",
    "    image_url = downloads.find(\"a\")[\"href\"]\n",
    "    hemisphere_image_urls.append({\"title\": title, \"img_url\": image_url})\n",
    "\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
